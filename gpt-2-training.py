# -*- coding: utf-8 -*-
"""
Automatically generated by Colaboratory.

#  Train a GPT-2 Text-Generating Model w/ GPU

adapted from (https://colab.research.google.com/drive/1VLG8e7YSEwypxU-noRNhsv5dW4NfTGce)

- finetuning gpt-2 model based on single-column csv input
- saving trained model
- using trained model for text-generation

"""

!pip install -q gpt-2-simple
import gpt_2_simple as gpt2
from datetime import datetime
from google.colab import files

"""## GPU
"""

!nvidia-smi

"""## Downloading GPT-2

If you're retraining a model on new text, you need to download the GPT-2 model first. 

There are three released sizes of GPT-2:

* `124M` (default): the "small" model, 500MB on disk.
* `355M`: the "medium" model, 1.5GB on disk.
* `774M`: the "large" model, cannot currently be finetuned with Colaboratory but can be used to generate text from the pretrained model (see later in Notebook)
* `1558M`: the "extra large", true model. Will not work if a K80/P4 GPU is attached to the notebook. (like `774M`, it cannot be finetuned).
"""

gpt2.download_gpt2(model_name="124M")

from google.colab import drive
drive.mount('/content/drive')

"""## Mounting Google Drive"""

gpt2.mount_gdrive()
file_name = "clean-data.csv"


"""## Finetune GPT-2
"""

sess = gpt2.start_tf_sess()

gpt2.finetune(sess,
              dataset=file_name,
              model_name='124M',
              steps=1000,
              restore_from='fresh',
              run_name='runFC',
              print_every=10,
              sample_every=200,
              save_every=500
              )

gpt2.copy_checkpoint_to_gdrive(run_name='runFC')


## Load a Trained Model Checkpoint

gpt2.copy_checkpoint_from_gdrive(run_name='runFC')
sess = gpt2.start_tf_sess()
gpt2.load_gpt2(sess, run_name='runFC')

# Generate Text From The Trained Model

gpt2.generate(sess, run_name='runFC', temperature=0.85, length=50, nsamples=10, return_as_list=True)